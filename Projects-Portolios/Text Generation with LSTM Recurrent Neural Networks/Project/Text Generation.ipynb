{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with LSTM Recurrent Neural Nerworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, a sample text file is used as an input for which a machine will be trained. Based on the training, machine will automatically execute its own generated words. The entire code is written in Python. Jupyter notebook is used as the interface to write the project.\n",
    "In order to move further with the coding part, first we need to learn more about Recurrent Neural Networks(RNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs) are popular models that are greatly used in many Natural Language Processing (NLP) tasks. A recurrent neural network (RNN) is a part of artificial neural network (ANN) where connections between units form a directed graph along a sequence. This allows it to exhibit dynamic temporal behavior for a time sequence. RNNs use their internal state to process the sequence of inputs. This is not possible in feedforward neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are RNNs exactly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs are neural networks which repeatedly make use of sequential information.The main assumption in a traditional artificial neural network is that all inputs and outputs are independent of each other and are not in sequence. But if we look at it, it is not such a great way to approach ANNs. Imagine a scenario where you have to predict the next song a user might want to listen. This will be impossible if the model won't know what songs have already been played. RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being dependent on the previous computations. Another way to think about RNNs is that they have a “memory” which captures information about what has been calculated so far. RNNs are supposed to make use of information in very long sequences but in reality, they can look back at only few steps. Here is what a typical RNN looks like:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://image.ibb.co/b5PWQo/download_26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can RNNs do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Language Modelling\n",
    "2. Image captioning\n",
    "3. Answering the questions\n",
    "4. Video to text conversion\n",
    "5. Image generation\n",
    "6. Sequence to sequence learning\n",
    "7. Speech recognition\n",
    "8. Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What RNNs cannot do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. RNNs have very high complexity as they are recurring. Meaning the keep on executing & training the model with the training results itself.\n",
    "2. RNNs are not stable. RNNs reinforce the feedback cycles on their own because in recurrent networks, the outputs are the inputs!\n",
    "3. There is a lot of noise generation\n",
    "4. They do not have terminal layer. They can keep training the data forever if they  wish.\n",
    "5. They cannot memorize large amount of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle the problem of memorizing large amounts of data, we use LSTM.\n",
    "LSTM are Long Short Term Memory networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM are special kind of Recurrent Neural Networks which are specifically designed to handle long term dependencies. They remember information for longer periods of time by default. It is not something they have to explicitly learn or train for. LSTM, like all RNNs have a chain like structure of repeating modules of neural network. LSTM has a different chain like structure as compared to traditional RNN. They have four neural network layers instead of one."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://image.ibb.co/dTTKBT/download_27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further demonstrate how LSTM actually works, we will perform text generation with a book as an input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we are trying to generate the next text that the machine will predict according to the inputs we provide.\n",
    "We are using Artificial neural networks instead of staistical models as we try to guess a human's behavior. The complexity of training an ANN is higher as compared to statistical models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the code as relative as possible, we have taken the following steps:\n",
    "    1. We install keras and tensorflow as it is not installed by default in Anaconda\n",
    "    2. Keep the filepaths relative & not absolute to the system\n",
    "    3. Attached the training file along the code in the package\n",
    "    4. Importing files and using aliases used globally to avoid confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing necessary packages including keras and tensorflow which needs to be installed\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the book which is in .txt format. We need to specify the encoding as it is required to make the file compatible in Python 3.X .Then we convert the .txt to all lower case. Converting to lower case will reduce the number of words the model will have to learn. We further plan to use the .txt file as it is to train the model with better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename,encoding=\"utf8\").read() # Need to specify the encoding in python 3+\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neural networks, it is easier to train integers as compared to characters. Hence, we convert the character values of the book to integers. First we create a set of all distict characters and then we map unique characters to unique integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the number of characters & the number of unique characters in the book. We find that there are around 163K characters and 61 unique characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163817\n",
      "Total Vocab:  61\n"
     ]
    }
   ],
   "source": [
    "# Now that the book has been loaded and the mapping prepared, we can summarize the dataset\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then divide the book in characters of length 100. In this way, the model will learn the next character after the 100 characters at a time and find out different patterns in the book. The more number of patterns that are discovered, better is the training set and will yield better results.\n",
    "\n",
    "For example, consider sequence length of 5 and word 'amazon'. It will first read 'amazo' and read 'n' after that.\n",
    "Consider a word greater than length 6. For example, 'wonderland'\n",
    "The reading for this word will be as follows:\n",
    "    1. 'wonde' & 'r'\n",
    "    2. 'onder' & 'l'\n",
    "    3. 'nderl' & 'a'\n",
    "    ... and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  163717\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns) # Don't indent!!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared our training data we need to transform it so that it is suitable for use with Keras.\n",
    "\n",
    "First we must transform the list of input sequences into the form expected by an LSTM network.\n",
    "\n",
    "Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize\n",
    "X = X / float(n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM is a modification of Recurrent Neural Network. Its performance is better than RNN because RNN is not capable of memorizing large amount of data. LSTM includes memory cell which can help maintain information in memory for longer amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no test dataset. We are modeling the entire training dataset to learn the probability of each character in a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is slow to train. Because of the slowness and because of our optimization requirements, we will use model checkpointing to record all of the network weights to file each time an improvement in loss is observed at the end of the epoch(one forward pass and one backward pass of all the training examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "163717/163717 [==============================] - 1101s 7ms/step - loss: 3.0122\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.01216, saving model to weights-improvement-01-3.0122.hdf5\n",
      "Epoch 2/50\n",
      "163717/163717 [==============================] - 1121s 7ms/step - loss: 2.8376\n",
      "\n",
      "Epoch 00002: loss improved from 3.01216 to 2.83761, saving model to weights-improvement-02-2.8376.hdf5\n",
      "Epoch 3/50\n",
      "163717/163717 [==============================] - 1098s 7ms/step - loss: 2.7549\n",
      "\n",
      "Epoch 00003: loss improved from 2.83761 to 2.75489, saving model to weights-improvement-03-2.7549.hdf5\n",
      "Epoch 4/50\n",
      "163717/163717 [==============================] - 1068s 7ms/step - loss: 2.6889\n",
      "\n",
      "Epoch 00004: loss improved from 2.75489 to 2.68886, saving model to weights-improvement-04-2.6889.hdf5\n",
      "Epoch 5/50\n",
      "163717/163717 [==============================] - 658s 4ms/step - loss: 2.6355\n",
      "\n",
      "Epoch 00005: loss improved from 2.68886 to 2.63550, saving model to weights-improvement-05-2.6355.hdf5\n",
      "Epoch 6/50\n",
      "163717/163717 [==============================] - 608s 4ms/step - loss: 2.5823\n",
      "\n",
      "Epoch 00006: loss improved from 2.63550 to 2.58227, saving model to weights-improvement-06-2.5823.hdf5\n",
      "Epoch 7/50\n",
      "163717/163717 [==============================] - 628s 4ms/step - loss: 2.5311\n",
      "\n",
      "Epoch 00007: loss improved from 2.58227 to 2.53114, saving model to weights-improvement-07-2.5311.hdf5\n",
      "Epoch 8/50\n",
      "163717/163717 [==============================] - 674s 4ms/step - loss: 2.4858\n",
      "\n",
      "Epoch 00008: loss improved from 2.53114 to 2.48582, saving model to weights-improvement-08-2.4858.hdf5\n",
      "Epoch 9/50\n",
      "163717/163717 [==============================] - 668s 4ms/step - loss: 2.4430\n",
      "\n",
      "Epoch 00009: loss improved from 2.48582 to 2.44300, saving model to weights-improvement-09-2.4430.hdf5\n",
      "Epoch 10/50\n",
      "163717/163717 [==============================] - 624s 4ms/step - loss: 2.4043\n",
      "\n",
      "Epoch 00010: loss improved from 2.44300 to 2.40433, saving model to weights-improvement-10-2.4043.hdf5\n",
      "Epoch 11/50\n",
      "163717/163717 [==============================] - 644s 4ms/step - loss: 2.3660\n",
      "\n",
      "Epoch 00011: loss improved from 2.40433 to 2.36604, saving model to weights-improvement-11-2.3660.hdf5\n",
      "Epoch 12/50\n",
      "163717/163717 [==============================] - 649s 4ms/step - loss: 2.3328\n",
      "\n",
      "Epoch 00012: loss improved from 2.36604 to 2.33282, saving model to weights-improvement-12-2.3328.hdf5\n",
      "Epoch 13/50\n",
      "163717/163717 [==============================] - 633s 4ms/step - loss: 2.2992\n",
      "\n",
      "Epoch 00013: loss improved from 2.33282 to 2.29921, saving model to weights-improvement-13-2.2992.hdf5\n",
      "Epoch 14/50\n",
      "163717/163717 [==============================] - 651s 4ms/step - loss: 2.2661\n",
      "\n",
      "Epoch 00014: loss improved from 2.29921 to 2.26606, saving model to weights-improvement-14-2.2661.hdf5\n",
      "Epoch 15/50\n",
      "163717/163717 [==============================] - 646s 4ms/step - loss: 2.2335\n",
      "\n",
      "Epoch 00015: loss improved from 2.26606 to 2.23347, saving model to weights-improvement-15-2.2335.hdf5\n",
      "Epoch 16/50\n",
      "163717/163717 [==============================] - 634s 4ms/step - loss: 2.2065\n",
      "\n",
      "Epoch 00016: loss improved from 2.23347 to 2.20646, saving model to weights-improvement-16-2.2065.hdf5\n",
      "Epoch 17/50\n",
      "163717/163717 [==============================] - 654s 4ms/step - loss: 2.1751\n",
      "\n",
      "Epoch 00017: loss improved from 2.20646 to 2.17505, saving model to weights-improvement-17-2.1751.hdf5\n",
      "Epoch 18/50\n",
      "163717/163717 [==============================] - 631s 4ms/step - loss: 2.1500\n",
      "\n",
      "Epoch 00018: loss improved from 2.17505 to 2.14997, saving model to weights-improvement-18-2.1500.hdf5\n",
      "Epoch 19/50\n",
      "163717/163717 [==============================] - 633s 4ms/step - loss: 2.1234\n",
      "\n",
      "Epoch 00019: loss improved from 2.14997 to 2.12341, saving model to weights-improvement-19-2.1234.hdf5\n",
      "Epoch 20/50\n",
      "163717/163717 [==============================] - 1189s 7ms/step - loss: 2.0979\n",
      "\n",
      "Epoch 00020: loss improved from 2.12341 to 2.09788, saving model to weights-improvement-20-2.0979.hdf5\n",
      "Epoch 21/50\n",
      "163717/163717 [==============================] - 1205s 7ms/step - loss: 2.0760\n",
      "\n",
      "Epoch 00021: loss improved from 2.09788 to 2.07603, saving model to weights-improvement-21-2.0760.hdf5\n",
      "Epoch 22/50\n",
      "163717/163717 [==============================] - 1245s 8ms/step - loss: 2.0546\n",
      "\n",
      "Epoch 00022: loss improved from 2.07603 to 2.05463, saving model to weights-improvement-22-2.0546.hdf5\n",
      "Epoch 23/50\n",
      "163717/163717 [==============================] - 1230s 8ms/step - loss: 2.0361\n",
      "\n",
      "Epoch 00023: loss improved from 2.05463 to 2.03614, saving model to weights-improvement-23-2.0361.hdf5\n",
      "Epoch 24/50\n",
      "163717/163717 [==============================] - 1194s 7ms/step - loss: 2.0231\n",
      "\n",
      "Epoch 00024: loss improved from 2.03614 to 2.02314, saving model to weights-improvement-24-2.0231.hdf5\n",
      "Epoch 25/50\n",
      "163717/163717 [==============================] - 635s 4ms/step - loss: 1.9996\n",
      "\n",
      "Epoch 00025: loss improved from 2.02314 to 1.99956, saving model to weights-improvement-25-1.9996.hdf5\n",
      "Epoch 26/50\n",
      "163717/163717 [==============================] - 622s 4ms/step - loss: 1.9870\n",
      "\n",
      "Epoch 00026: loss improved from 1.99956 to 1.98703, saving model to weights-improvement-26-1.9870.hdf5\n",
      "Epoch 27/50\n",
      "163717/163717 [==============================] - 631s 4ms/step - loss: 1.9681\n",
      "\n",
      "Epoch 00027: loss improved from 1.98703 to 1.96805, saving model to weights-improvement-27-1.9681.hdf5\n",
      "Epoch 28/50\n",
      "163717/163717 [==============================] - 652s 4ms/step - loss: 1.9535\n",
      "\n",
      "Epoch 00028: loss improved from 1.96805 to 1.95353, saving model to weights-improvement-28-1.9535.hdf5\n",
      "Epoch 29/50\n",
      "163717/163717 [==============================] - 621s 4ms/step - loss: 1.9383\n",
      "\n",
      "Epoch 00029: loss improved from 1.95353 to 1.93829, saving model to weights-improvement-29-1.9383.hdf5\n",
      "Epoch 30/50\n",
      "163717/163717 [==============================] - 631s 4ms/step - loss: 1.9298\n",
      "\n",
      "Epoch 00030: loss improved from 1.93829 to 1.92984, saving model to weights-improvement-30-1.9298.hdf5\n",
      "Epoch 31/50\n",
      "163717/163717 [==============================] - 640s 4ms/step - loss: 1.9153\n",
      "\n",
      "Epoch 00031: loss improved from 1.92984 to 1.91530, saving model to weights-improvement-31-1.9153.hdf5\n",
      "Epoch 32/50\n",
      "163717/163717 [==============================] - 631s 4ms/step - loss: 1.9058\n",
      "\n",
      "Epoch 00032: loss improved from 1.91530 to 1.90582, saving model to weights-improvement-32-1.9058.hdf5\n",
      "Epoch 33/50\n",
      "163717/163717 [==============================] - 637s 4ms/step - loss: 1.8917\n",
      "\n",
      "Epoch 00033: loss improved from 1.90582 to 1.89173, saving model to weights-improvement-33-1.8917.hdf5\n",
      "Epoch 34/50\n",
      "163717/163717 [==============================] - 629s 4ms/step - loss: 1.8845\n",
      "\n",
      "Epoch 00034: loss improved from 1.89173 to 1.88449, saving model to weights-improvement-34-1.8845.hdf5\n",
      "Epoch 35/50\n",
      "163717/163717 [==============================] - 640s 4ms/step - loss: 1.8708\n",
      "\n",
      "Epoch 00035: loss improved from 1.88449 to 1.87080, saving model to weights-improvement-35-1.8708.hdf5\n",
      "Epoch 36/50\n",
      "163717/163717 [==============================] - 635s 4ms/step - loss: 1.8600\n",
      "\n",
      "Epoch 00036: loss improved from 1.87080 to 1.86001, saving model to weights-improvement-36-1.8600.hdf5\n",
      "Epoch 37/50\n",
      "163717/163717 [==============================] - 636s 4ms/step - loss: 2.1130\n",
      "\n",
      "Epoch 00037: loss did not improve\n",
      "Epoch 38/50\n",
      "163717/163717 [==============================] - 659s 4ms/step - loss: 1.9774\n",
      "\n",
      "Epoch 00038: loss did not improve\n",
      "Epoch 39/50\n",
      "163717/163717 [==============================] - 633s 4ms/step - loss: 1.8809\n",
      "\n",
      "Epoch 00039: loss did not improve\n",
      "Epoch 40/50\n",
      "163717/163717 [==============================] - 637s 4ms/step - loss: 1.8394\n",
      "\n",
      "Epoch 00040: loss improved from 1.86001 to 1.83940, saving model to weights-improvement-40-1.8394.hdf5\n",
      "Epoch 41/50\n",
      "163717/163717 [==============================] - 673s 4ms/step - loss: 1.8219\n",
      "\n",
      "Epoch 00041: loss improved from 1.83940 to 1.82195, saving model to weights-improvement-41-1.8219.hdf5\n",
      "Epoch 42/50\n",
      "163717/163717 [==============================] - 645s 4ms/step - loss: 1.8182\n",
      "\n",
      "Epoch 00042: loss improved from 1.82195 to 1.81823, saving model to weights-improvement-42-1.8182.hdf5\n",
      "Epoch 43/50\n",
      "163717/163717 [==============================] - 642s 4ms/step - loss: 1.8205\n",
      "\n",
      "Epoch 00043: loss did not improve\n",
      "Epoch 44/50\n",
      "163717/163717 [==============================] - 625s 4ms/step - loss: 1.8212\n",
      "\n",
      "Epoch 00044: loss did not improve\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163717/163717 [==============================] - 605s 4ms/step - loss: 1.8178\n",
      "\n",
      "Epoch 00045: loss improved from 1.81823 to 1.81778, saving model to weights-improvement-45-1.8178.hdf5\n",
      "Epoch 46/50\n",
      "163717/163717 [==============================] - 625s 4ms/step - loss: 1.8171\n",
      "\n",
      "Epoch 00046: loss improved from 1.81778 to 1.81712, saving model to weights-improvement-46-1.8171.hdf5\n",
      "Epoch 47/50\n",
      "163717/163717 [==============================] - 636s 4ms/step - loss: 1.8089\n",
      "\n",
      "Epoch 00047: loss improved from 1.81712 to 1.80894, saving model to weights-improvement-47-1.8089.hdf5\n",
      "Epoch 48/50\n",
      "163717/163717 [==============================] - 639s 4ms/step - loss: 2.1773\n",
      "\n",
      "Epoch 00048: loss did not improve\n",
      "Epoch 49/50\n",
      "163717/163717 [==============================] - 656s 4ms/step - loss: 2.1168\n",
      "\n",
      "Epoch 00049: loss did not improve\n",
      "Epoch 50/50\n",
      "163717/163717 [==============================] - 663s 4ms/step - loss: 1.9326\n",
      "\n",
      "Epoch 00050: loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1108b10b828>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, batch_size=128, callbacks=callbacks_list) \n",
    "#This takes a lot of time(5 minutes per epoch due to batch size of 512 patterns).\n",
    "#For better test results/future scope, we can increase the size of epoch with reduced batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-47-1.8089.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" ck turtle’s\n",
      "heavy sobs.\n",
      "\n",
      "lastly, she pictured to herself how this same little sister of hers\n",
      "would,  \"\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "import sys\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and the whrt on thet wise she wes oo the thet had bed an a leeuter wote. and the was nor in anliher wored in a little worde. and she taid to herself ‘iht sae a citters tas a aonser tf that io the sabe. and the west on tuiee to an and then at the codless, and she was norking the thiet oaed th the was to gend her haad. \n",
      "‘he io wonh th the wout!’ she kact rhidpedd in a tory of tiice of the gormh. \n",
      "‘ht is sie suree ihtee bn a les lis wo teye you mo youh tees the sea--’\n",
      "\n",
      "‘what woul toeete,’ said the match hare.\n",
      "\n",
      "‘ih toete thee io the wes,’ said alice, ‘io would be a very oo tuee an all an it eane  and i saad io so teet to teee ’o\n",
      "\n",
      "al cel toe white sited the dooreuse the dad not thee to tee otoersins oo the that was a lott of linel to bn the wint wire an in hade an herse fer and taed to the thett whine whs dad boen the shing then soeetien, and the woide the was not in a lintte or two she was notking an it was tore tie was thet hard sas she winle rai her hand and a lroked oo the tioed the was\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index) # To append a single remaining index value\n",
    "    pattern = pattern[1:len(pattern)] # To remove an extra index value\n",
    "print(\"\\nDone.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train the model more by increasing the epochs & decreasing the batch size\n",
    "2. Try different RNN and compare which is better\n",
    "3. Try to train the data with more than 1 sources\n",
    "4. Predict less characters for output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Licenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is licensed under MIT which is available on the GitHub site which we created: \n",
    "https://github.com/luvesht/Machine-Learning/blob/master/LICENSE\n",
    "\n",
    "Also, we have referred Trung Tran's work and following is his license.\n",
    "https://github.com/ChunML/text-generator/blob/master/LICENSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "2. https://www.datacamp.com/community/tutorials/deep-learning-python\n",
    "3. https://github.com/tensorflow/models\n",
    "4. https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/\n",
    "5. https://en.wikipedia.org/wiki/Long_short-term_memory\n",
    "6. https://deeplearning4j.org/lstm.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
